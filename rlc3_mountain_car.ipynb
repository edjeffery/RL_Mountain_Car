{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM50270 Reinforcement Learning: Coursework 3 (Mountain Car)\n",
    "\n",
    "Please remember: \n",
    "(1) Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing.\n",
    "(2) Save your work regularly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Code for Mountain Car \n",
    "\n",
    "We provide a `MountainCar` class that you can use. This implementation is based on the problem description given in [Example 8.2](http://www.incompleteideas.net/book/ebook/node89.html) of Sutton & Barto (1998) The following cells in this section will walk you through the basic usage of this class.\n",
    "\n",
    "We import the mountaincar module and create a `MountainCar` object called `env`. The `reset()` method chooses a random starting `position` and starting `velocity` for the car, and sets the `game_over` variable to `False`. You can access these state variables independently using the same names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mountaincar\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "env = mountaincar.MountainCar()\n",
    "env.reset()\n",
    "print(\"Starting position of the car\", env.position)\n",
    "print(\"Starting velocity of the car\", env.velocity)\n",
    "if not env.game_over:\n",
    "    print(\"Game is not over yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the current position of the car using the `plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with the `MountainCar` environment using the `make_step()` method. This method takes an `action` as input and computes the response of the environment. This method returns a `reward` signal, which is always -1.\n",
    "\n",
    "The action can be one of the following integers:\n",
    "* -1: full throttle reverse\n",
    "*  0: zero throttle\n",
    "*  1: full throttle forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drive a bit full throttle forward and plot again.\n",
    "env.make_step(action=1)\n",
    "env.make_step(action=1)\n",
    "env.make_step(action=1)\n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet shows that even at full throttle the car cannot accelerate up the steep slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 150\n",
    "for episode in range(num_steps):\n",
    "    # Always action 1 (full throttle forward)\n",
    "    env.make_step(action=1)\n",
    "    env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (50 marks):\n",
    "\n",
    "For your reference, the pseudo-code for  _Linear, gradient-descent Sarsa($\\lambda$)_ is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, [Section 8.4](http://www.incompleteideas.net/book/ebook/node89.html#fig:FAsarsa).\n",
    "<img src=\"images/gradient_descent_Sarsa.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Please plot an average learning curve for your agent. This should be a static figure of _precomputed_ results, clearly showing (1) how efficiently an average agent learns, and (2) how good the eventual policy is. In five sentences or less, describe your choice of parameter settings and your results.\n",
    "\n",
    "In addition, please write code to produce a learning curve for a _single_ agent. This shoud be a dynamic figure that we can produce from scratch by executing your code. This figure can show less detail than the static plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mountaincar\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SarsaAgent():\n",
    "    \n",
    "    def __init__(self, alpha, gamma, epsilon, lambda_):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "        self.theta = np.zeros((10,10,10,3))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.theta = np.zeros((10,10,10,3))\n",
    "    \n",
    "    def choose_action(self, state, actions, tilings):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = random.choice(actions)\n",
    "            Q = self.calcQ(state, tilings)\n",
    "            #print(\"Random. Index =\", action + 1)\n",
    "            return action, Q[action + 1]\n",
    "        else:\n",
    "            Q = self.calcQ(state, tilings)\n",
    "            maxQ = max(Q)\n",
    "            #print(\"Max Q:\", maxQ)\n",
    "            if (Q == maxQ).sum() > 1:\n",
    "                best = [i for i in range(len(actions)) if Q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "                #print(\"RC i =\", i)\n",
    "            else:\n",
    "                #i_np = np.where(Q == maxQ)\n",
    "                #i = np.asscalar(i_np[0])\n",
    "                i = np.argmax(Q)\n",
    "                #print(\"NP where i =\", i)\n",
    "\n",
    "        action = actions[i]\n",
    "        #print(\"Greedy. Index =\", action + 1)\n",
    "        return action, Q[action + 1] # +1 \n",
    "\n",
    "    def choose_random_action(self, actions):\n",
    "        return random.choice(actions)\n",
    "    \n",
    "    def calcQ(self, state, tilings):\n",
    "        Q = np.zeros(3)\n",
    "        for a in [-1, 0, 1]:\n",
    "            set_of_features = tileCode(state, a, tilings)\n",
    "            Q[a + 1] = Q[a + 1] + np.sum(self.theta * set_of_features)\n",
    "        #print(\"Q:\", Q)\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to separate this into two functions: one for creating the tiling,\n",
    "# and one for getting the features for a (state, action) pair\n",
    "def createTiling():\n",
    "    #position = state[0]\n",
    "    #velocity = state[1]\n",
    "    \n",
    "    num_tiles = 10\n",
    "    min_x = -1.2\n",
    "    max_x = 0.5\n",
    "    x_tile_width = (max_x - min_x) / (num_tiles - 2)\n",
    "    min_y = -0.07\n",
    "    max_y = 0.07\n",
    "    y_tile_width = (max_y - min_y) / (num_tiles - 2)\n",
    "    \n",
    "    #tiles = np.zeros((10,4), dtype=np.int)\n",
    "    tilings = np.zeros((10, 2, 10))\n",
    "    \n",
    "    #if -1.2 <= position <= 0.5 and -0.07 <= velocity <= 0.07:\n",
    "    \n",
    "    for tiling in range(num_tiles):\n",
    "        x_offset = np.random.uniform(0, x_tile_width)\n",
    "        #print(\"X offset:\", x_offset)\n",
    "        y_offset = np.random.uniform(0, y_tile_width)     \n",
    "        xs = np.linspace(min_x, max_x + x_tile_width, num_tiles) - x_offset\n",
    "        ys = np.linspace(min_y, max_y + y_tile_width, num_tiles) - y_offset\n",
    "        tilings[tiling] = np.array([xs, ys])\n",
    "        \n",
    "    return tilings\n",
    "\n",
    "def tileCode(state, action, tilings):\n",
    "    position = np.array(state[0])\n",
    "    velocity = np.array(state[1])\n",
    "    num_tiles = len(tilings)\n",
    "    #tiles = np.zeros((10,4), dtype=np.int)\n",
    "    tiles_ = np.zeros((10,10,10,3))\n",
    "    for tiling in range(num_tiles):\n",
    "        xs = tilings[tiling][0]\n",
    "        ys = tilings[tiling][1]\n",
    "        xi = np.digitize(position, xs)\n",
    "        yi = np.digitize(velocity, ys)\n",
    "        #print(xi, yi)\n",
    "        #feature = np.array([xi, yi, tiling, action])\n",
    "        #print(feature)\n",
    "        #tiles[tiling] = feature\n",
    "        tiles_[xi, yi, tiling, action] = 1\n",
    "    return tiles_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([env.position, env.velocity])\n",
    "action = 0\n",
    "tilings = createTiling()\n",
    "print(tilings)\n",
    "set_of_features = tileCode(state, action, tilings)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(10):\n",
    "            for l in range(3):\n",
    "                if set_of_features[i, j, k, l] == 1:\n",
    "                    print(i, j, k, l)\n",
    "#print(set_of_features)\n",
    "#i = set_of_features[0]\n",
    "#sarsa_agent = SarsaAgent(alpha=0.1, gamma=1.0, epsilon=0.05, lambda_=0.9)\n",
    "#print(sarsa_agent.theta[i[0], i[1], i[2], i[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def play(env, sarsa_agent, num_episodes, plot=False):\n",
    "    alpha = sarsa_agent.alpha\n",
    "    gamma = sarsa_agent.gamma\n",
    "    lambda_ = sarsa_agent.lambda_\n",
    "    \n",
    "    reward_per_episode = np.zeros(num_episodes)\n",
    "    steps_per_episode = np.zeros(num_episodes)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        cumulative_reward = 0\n",
    "        step = 0\n",
    "        #print(\"Episode\", episode)\n",
    "        e = np.zeros((10, 10, 10, 3))\n",
    "        env.reset()\n",
    "        state = np.array([env.position, env.velocity])\n",
    "        action = sarsa_agent.choose_random_action(env.actions)\n",
    "        tilings = createTiling()\n",
    "#        set_of_features = tileCode(state, action, tilings)\n",
    "#        print(set_of_features)\n",
    "        \n",
    "        while not env.game_over:\n",
    "            #print(\"State\", state, \"Action:\", action)\n",
    "            set_of_features = tileCode(state, action, tilings)\n",
    "            e = e + set_of_features # accumulating traces\n",
    "            # make step and get reward and next state\n",
    "            reward = env.make_step(action)\n",
    "            new_state = np.array([env.position, env.velocity])\n",
    "            # calculate Q and then TD error\n",
    "            Q = sarsa_agent.calcQ(new_state, tilings)\n",
    "            delta = reward - Q[action + 1] # add 1 to action to get index (-1, 0, 1)->(0,1,2)\n",
    "            action, Qa = sarsa_agent.choose_action(new_state, env.actions, tilings)\n",
    "            #print(Qa)\n",
    "            delta = delta + gamma * Qa\n",
    "            sarsa_agent.theta = sarsa_agent.theta + alpha * delta * e\n",
    "            e = gamma * lambda_ * e\n",
    "            state = np.copy(new_state)\n",
    "            step += 1\n",
    "            cumulative_reward += reward\n",
    "            if plot == True:\n",
    "                env.plot()\n",
    "        reward_per_episode[episode] = cumulative_reward\n",
    "        steps_per_episode[episode] = step\n",
    "    print(\"complete\")\n",
    "    return reward_per_episode, steps_per_episode\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 2\n",
    "num_episodes = 500\n",
    "all_rewards = np.zeros(num_episodes)\n",
    "all_steps = np.zeros(num_episodes)\n",
    "for i in range(n_agents):\n",
    "    sarsa_agent = SarsaAgent(alpha=0.1, gamma=1.0, epsilon=0.05, lambda_=0.9)\n",
    "    env = mountaincar.MountainCar()\n",
    "    rewards, steps = play(env, sarsa_agent, num_episodes, plot=False)\n",
    "    all_rewards += rewards\n",
    "    all_steps += steps\n",
    "\n",
    "all_rewards = all_rewards / n_agents\n",
    "all_steps = all_steps / n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average function is not mine and was taken from the following link:\n",
    "# https://stackoverflow.com/questions/14313510/how-to-calculate-moving-average-using-numpy\n",
    "# Credit: Jaime\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, sarsa_agent, num_episodes=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(all_rewards))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0e6876eaaf6b2a30ed8b4f16988e03ed",
     "grade": true,
     "grade_id": "cell-13765e8707846b42",
     "locked": false,
     "points": 50,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Please make sure that all of your code is above this cell. Here, please insert your static learning curve and answer the verbal questions (describe your choice of parameters and results). \n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (50 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code in this cell. You can add additional code cells below this one, if you would like to.\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b3bb13522fc810fe1cd984ce0825a666",
     "grade": true,
     "grade_id": "cell-4fc1516337f6c8b8",
     "locked": false,
     "points": 50,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Please make sure that all of your code is above this cell. Please insert a static learning curve, showing the performance of your learning over time, and answer the questions provided on the coursework specification. Instead of providing your answer here, in this Jupyter notebook, you have the option to provide your answers on a separate pdf document, not exceeding two pages in length. If you do so, please write \"Answer in pdf file.\" in this cell.  \n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
